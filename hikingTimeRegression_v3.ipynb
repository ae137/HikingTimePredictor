{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "# Predict hiking times based on statistics from GPX files\n",
    "\n",
    "We attempt to predict the walking times and durations of hikes based on statistics extracted from GPX files. Using personal GPX records as input, the outputs are personalized as well.\n",
    "\n",
    "We use the `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import gpx_stats\n",
    "import utils\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### Load data\n",
    "Load the dataset that was prepared by running the following command in a terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9kxxgzvzlyz"
   },
   "source": [
    "`python prepareData.py '~/GPX-Tracks' 'Wandern'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9kxxgzvzlyz"
   },
   "outputs": [],
   "source": [
    "train_dataset_file = 'hiking_data_training.hdf5'\n",
    "test_dataset_file = 'hiking_data_test.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nslsRLh7Zss4"
   },
   "source": [
    "Convert data to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_to_dict(file_name):\n",
    "    hdf5_data = h5py.File(file_name, 'r')\n",
    "    hdf5_data_dict = {}\n",
    "    for name in gpx_stats.GpxSegmentStats.getHeader():\n",
    "        if name == 'Path':\n",
    "            hdf5_data_dict[name] = [gpx_stats.PathFeature(data) for data in hdf5_data[name]]\n",
    "        else:\n",
    "            hdf5_data_dict[name] = hdf5_data[name][...]\n",
    "\n",
    "    hdf5_data.close()\n",
    "    return hdf5_data_dict\n",
    "\n",
    "train_hdf5_data_dict = read_hdf5_to_dict(train_dataset_file)\n",
    "test_hdf5_data_dict = read_hdf5_to_dict(test_dataset_file)\n",
    "\n",
    "path_features_shape = train_hdf5_data_dict['Path'][0].shape\n",
    "    \n",
    "train_dataset = pd.DataFrame.from_dict(train_hdf5_data_dict)\n",
    "test_dataset = pd.DataFrame.from_dict(test_hdf5_data_dict)\n",
    "\n",
    "train_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4ubs136WLNp"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[[\"Duration\", \"Length2d\", \"Length3d\", \"MovingTime\", \"StoppedTime\", \"TotalUphill\", \"TotalDownhill\"]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "These plots reveal that all features extracted from the GPX track segments show some correlation with the duration and moving time.\n",
    "\n",
    "Also look at the overall statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "train_dataset_stats = train_dataset.describe()\n",
    "train_dataset_stats.pop(\"MovingTime\")\n",
    "train_dataset_stats.pop(\"StoppedTime\")\n",
    "train_dataset_stats.pop(\"Duration\")\n",
    "train_dataset_stats = train_dataset_stats.transpose()\n",
    "train_dataset_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Separate the target value, or \"label\", from the features. This label is the value that we will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "label_columns = ['MovingTime']\n",
    "drop_columns = [\"StoppedTime\", \"Duration\"]\n",
    "train_dataset.drop(columns=drop_columns, inplace=True)\n",
    "test_dataset.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "train_labels_data = []\n",
    "test_labels_data = []\n",
    "\n",
    "for col_name in label_columns:\n",
    "    train_labels_data.append(train_dataset.pop(col_name))\n",
    "    test_labels_data.append(test_dataset.pop(col_name))\n",
    "\n",
    "train_labels = pd.concat(train_labels_data, axis=1)\n",
    "train_labels.sort_index(inplace=True)\n",
    "test_labels = pd.concat(test_labels_data, axis=1)\n",
    "test_labels.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "The statistics about `train_stats` in the above block shows a wide variation of ranges for all features. Although a model *might* converge without feature normalization, the latter usually improves convergence properties.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution as the one the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - train_dataset_stats['mean']) / train_dataset_stats['std']\n",
    "\n",
    "train_paths = train_dataset.pop('Path')\n",
    "normed_train_data = norm(train_dataset)\n",
    "\n",
    "test_paths = test_dataset.pop('Path')\n",
    "normed_test_data = norm(test_dataset)\n",
    "\n",
    "train_paths_as_array = gpx_stats.convert_paths_to_array(train_paths)\n",
    "test_paths_as_array = gpx_stats.convert_paths_to_array(test_paths)\n",
    "\n",
    "# TODO: Properly fix NaN issue in elevation!!!\n",
    "# Quick and dirty fix: Set NaN values to 0\n",
    "train_paths_as_array = np.nan_to_num(train_paths_as_array, copy=False)\n",
    "test_paths_as_array = np.nan_to_num(test_paths_as_array, copy=False)\n",
    "\n",
    "with open('train_dataset_stats.csv', 'w') as csvfile:\n",
    "    train_dataset_stats.to_csv(csvfile, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuiClDk45eS4"
   },
   "source": [
    "`normed_train_data` is what we will use to train the model.\n",
    "\n",
    "Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model. This includes the test dataset as well as input during inference. Thus we need to save the normalization numbers together with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "## The model\n",
    "\n",
    "### Build the model\n",
    "\n",
    "Let's build our model. In this notebook, the architectures from the other two notebooks are combined: The statistical features are processed with fully-connected layers and the path features with LSTM layers. The resulting outputs are concatenated and processed with three more dense layers that finally output duration, moving time and stopped time. The model building steps are wrapped in a function, `build_model`, for convencience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "def build_model():\n",
    "    stat_inputs = layers.Input(shape=(normed_train_data.shape[1],), name='StatInput')\n",
    "    stat_dense1 = layers.Dense(16, activation=tf.nn.relu)(stat_inputs)\n",
    "    stat_dense2 = layers.Dense(16, activation=tf.nn.relu)(stat_dense1)\n",
    "    stat_dense3 = layers.Dense(16, activation=tf.nn.relu)(stat_dense2)\n",
    "    stat_dense4 = layers.Dense(16, activation=tf.nn.relu)(stat_dense3)\n",
    "    \n",
    "    path_inputs = layers.Input(shape=path_features_shape, name='PathInputs')\n",
    "    conv1 = layers.Conv1D(filters=6, kernel_size=3, padding='same', activation=tf.nn.relu)(path_inputs)\n",
    "    conv2 = layers.Conv1D(filters=12, kernel_size=3, padding='same', activation=tf.nn.relu)(conv1)\n",
    "    bn1 = layers.BatchNormalization()(conv2)\n",
    "    conv3 = layers.Conv1D(filters=24, kernel_size=3, padding='same', activation=tf.nn.relu)(bn1)\n",
    "    conv4 = layers.Conv1D(filters=6, kernel_size=3, padding='same', activation=tf.nn.relu)(conv3)\n",
    "    bn2 = layers.BatchNormalization()(conv4)\n",
    "    \n",
    "    lstm1 = layers.LSTM(16, return_sequences=True)(bn2)\n",
    "    dropout = layers.Dropout(0.25)(lstm1)\n",
    "    lstm2 = layers.LSTM(16, return_sequences=False)(dropout)\n",
    "    dropout2 = layers.Dropout(0.25)(lstm2)\n",
    "    lstm_dense1 = layers.Dense(16, activation=tf.nn.relu)(dropout2)\n",
    "    \n",
    "    concat = layers.Concatenate()([stat_dense4, lstm_dense1])\n",
    "    dense1 = layers.Dense(32, activation=tf.nn.relu)(concat)\n",
    "    dense2 = layers.Dense(32, activation=tf.nn.relu)(dense1)\n",
    "    \n",
    "    outputs = layers.Dense(len(train_labels.keys()), activation=None)(dense2)\n",
    "    \n",
    "    model = models.Model(inputs=[stat_inputs, path_inputs], outputs=outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6W50qGsJAL"
   },
   "source": [
    "\n",
    "Now try out the model. Take a batch of `4` examples from the training data and call `model.predict` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d-gBaVtGTSC"
   },
   "outputs": [],
   "source": [
    "example_batch = [normed_train_data[:4], train_paths_as_array[:4]]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Train the model for up to 500 epochs with the training set, and record the training and validation accuracy in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                 patience=20, min_lr=0.01*learning_rate)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=1, \n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping]\n",
    "\n",
    "\n",
    "history = model.fit([normed_train_data, train_paths_as_array], \n",
    "                    train_labels.values,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=([normed_test_data, test_paths_as_array], test_labels),\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "Visualize the model's training progress using the stats stored in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Xj91b-dymEy"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XriGbVPh2t"
   },
   "outputs": [],
   "source": [
    "utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft603OzXuEZC"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "Finally, predict walking time values using data in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "compute_standard_walking_time_vectorized = np.vectorize(utils.compute_standard_walking_time)\n",
    "\n",
    "standard_estimate_walking_time = compute_standard_walking_time_vectorized(test_dataset['Length2d'].values,\n",
    "                                                                          test_dataset['TotalUphill'].values,\n",
    "                                                                          test_dataset['TotalDownhill'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict([normed_test_data, test_paths_as_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "for i, label_col_name in enumerate(label_columns):\n",
    "    utils.scatter_plot(test_labels[label_col_name],\n",
    "                       test_predictions[:, i],\n",
    "                       label_col_name,\n",
    "                       max_val=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrkHGKZcusUo"
   },
   "source": [
    "It looks like our model predicts durations and moving times reasonably well, as the results should be as close to the diagonal as possible. However, it has quite some problems with predicting the stopped times. Let's take a look at the error distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label_col_name in enumerate(label_columns):\n",
    "    utils.plot_error_hist(test_labels[label_col_name], test_predictions[:, i], label_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9_kI6MHu1UU"
   },
   "source": [
    "It's not quite gaussian, but we might expect that because the number of samples is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_predictions[:, i], alpha=0.5, density=False, label=\"Pred\", bins=50)\n",
    "plt.hist(test_labels[label_col_name], alpha=0.5, density=False, label=\"True\", bins=50)\n",
    "plt.xlabel(\"Moving time [s]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([normed_test_data, test_paths_as_array], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9_kI6MHu1UU"
   },
   "source": [
    "Export model for use in inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_hikingTimePrediction_mixed.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook presents the training of an algorithm for predicting moving and stopping times as well as total duration for hiking (but it can also be applied to many other outdoor activities). The model that takes statistical features and path features as inputs performs a bit better than the other two architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

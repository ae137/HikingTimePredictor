{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "# Predict hiking times based on statistics from GPX files\n",
    "\n",
    "We attempt to predict the walking times and durations of hikes based on statistics extracted from GPX files. Using personal GPX records as input, the outputs are personalized as well.\n",
    "\n",
    "The code is partially based on the `basic_regression.ipynb` notebook from [Tensorflow tutorials](https://github.com/tensorflow/docs/blob/r1.13/site/en/tutorials/keras/basic_regression.ipynb).\n",
    "\n",
    "We use the `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import gpx_stats\n",
    "import utils\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### Load data\n",
    "\n",
    "Load the dataset that was prepared by running the following command in a shell:\n",
    "\n",
    "`run prepareData.py '~/GPX-Tracks' 'Wandern'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9kxxgzvzlyz"
   },
   "outputs": [],
   "source": [
    "train_dataset_file = 'hiking_data_training.hdf5'\n",
    "test_dataset_file = 'hiking_data_test.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nslsRLh7Zss4"
   },
   "source": [
    "Convert data to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5_to_dict(file_name):\n",
    "    hdf5_data = h5py.File(file_name, 'r')\n",
    "    hdf5_data_dict = {}\n",
    "    for name in gpx_stats.GpxSegmentStats.get_header():\n",
    "        if name == 'Path':\n",
    "            hdf5_data_dict[name] = [gpx_stats.PathFeature(data) for data in hdf5_data[name]]\n",
    "        else:\n",
    "            hdf5_data_dict[name] = hdf5_data[name][...]\n",
    "\n",
    "    hdf5_data.close()\n",
    "    return hdf5_data_dict\n",
    "\n",
    "train_hdf5_data_dict = read_hdf5_to_dict(train_dataset_file)\n",
    "test_hdf5_data_dict = read_hdf5_to_dict(test_dataset_file)\n",
    "\n",
    "train_path_features_shape = train_hdf5_data_dict['Path'][0].shape\n",
    "test_path_features_shape = test_hdf5_data_dict['Path'][0].shape\n",
    "    \n",
    "train_dataset = pd.DataFrame.from_dict(train_hdf5_data_dict)\n",
    "train_dataset.pop('Path')\n",
    "test_dataset = pd.DataFrame.from_dict(test_hdf5_data_dict)\n",
    "test_dataset.pop('Path')\n",
    "\n",
    "\n",
    "train_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4ubs136WLNp"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[[\"Duration\", \"Length2d\", \"Length3d\", \"MovingTime\", \"StoppedTime\", \"TotalUphill\", \"TotalDownhill\"]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "All features show some correlation with the moving time and duration.\n",
    "\n",
    "Also look at the overall feature statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "train_dataset_stats = train_dataset.describe()\n",
    "train_dataset_stats.pop(\"MovingTime\")\n",
    "train_dataset_stats.pop(\"StoppedTime\")\n",
    "train_dataset_stats.pop(\"Duration\")\n",
    "train_dataset_stats = train_dataset_stats.transpose()\n",
    "train_dataset_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Separate the target value, or \"label\", from the features. This label is the value that we will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "label_columns = ['MovingTime', 'StoppedTime', 'Duration']\n",
    "train_labels_data = []\n",
    "test_labels_data = []\n",
    "\n",
    "for col_name in label_columns:\n",
    "    train_labels_data.append(train_dataset.pop(col_name))\n",
    "    test_labels_data.append(test_dataset.pop(col_name))\n",
    "\n",
    "train_labels = pd.concat(train_labels_data, axis=1)\n",
    "train_labels.sort_index(inplace=True)\n",
    "test_labels = pd.concat(test_labels_data, axis=1)\n",
    "test_labels.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.tail())\n",
    "print(train_labels.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "The statistics about `train_dataset_stats` in the above block shows a wide variation of ranges for all features. Although a model *might* converge without feature normalization, the latter usually improves convergence properties.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution as the one the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - train_dataset_stats['mean']) / train_dataset_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)\n",
    "\n",
    "with open('train_stats_simple.csv', 'w') as csvfile:\n",
    "    train_dataset_stats.to_csv(csvfile, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuiClDk45eS4"
   },
   "source": [
    "`normed_train_data` is what we will use to train the model.\n",
    "\n",
    "Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model. This includes the test dataset as well as input during inference. Thus we need to save the normalization numbers together with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "## The model\n",
    "\n",
    "### Build the model\n",
    "\n",
    "Let's build our model. In this notebook, we use a very simple multi-layer perceptron with a few fully connected layers. The last layer hast three units without activation function, suitable for a regression problem. for predicting duration, moving time and stopped time. The model building steps are wrapped in a function, `build_model`, for convencience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "\n",
    "def build_model():\n",
    "    statistics_inputs = layers.Input(shape=(train_dataset.shape[1],), name='StatisticsInput')\n",
    "    dense1 = layers.Dense(32, activation=tf.nn.relu)(statistics_inputs)\n",
    "    dense2 = layers.Dense(32, activation=tf.nn.relu)(dense1)\n",
    "    dense3 = layers.Dense(32, activation=tf.nn.relu)(dense2)\n",
    "    dense4 = layers.Dense(32, activation=tf.nn.relu)(dense3)\n",
    "    outputs = layers.Dense(len(train_labels.keys()), activation=None)(dense4)\n",
    "    \n",
    "    model = models.Model(inputs=statistics_inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sj49Og4YGULr"
   },
   "source": [
    "How did we specify the architecture of the model?\n",
    "\n",
    "The architecture is chosen such that we can reproduce esimates for walking times from a standard formula that is implemented in the function `compute_standard_walking_time` in `utils.py`.\n",
    "\n",
    "### Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReAD0n6MsFK-"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6W50qGsJAL"
   },
   "source": [
    "\n",
    "Now try out the model. Take a batch of `4` examples from the training data and call `model.predict` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d-gBaVtGTSC"
   },
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:4]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Train the model for up to 500 epochs with the training set, and record the training and validation accuracy in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                 patience=20, min_lr=0.01*learning_rate)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=1, \n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping]\n",
    "\n",
    "\n",
    "history = model.fit(normed_train_data.values, train_labels.values,\n",
    "                    epochs=EPOCHS, validation_split=0.2, verbose=1,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "Visualize the model's training progress using the stats stored in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Xj91b-dymEy"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XriGbVPh2t"
   },
   "outputs": [],
   "source": [
    "utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft603OzXuEZC"
   },
   "source": [
    "As our data set of real GPX tracks is very small, the optimal choice of the epoch for ending training depends on the distribution of tracks between training, validation and testing data. The above choice led to quite stable results.\n",
    "\n",
    "\n",
    "### Make predictions\n",
    "\n",
    "Finally, predict walking time values using data in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "compute_standard_walking_time_vectorized = np.vectorize(utils.compute_standard_walking_time)\n",
    "\n",
    "standard_estimate_walking_time = compute_standard_walking_time_vectorized(test_dataset['Length2d'].values,\n",
    "                                                                          test_dataset['TotalUphill'].values,\n",
    "                                                                          test_dataset['TotalDownhill'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "moving_stopped_duration_data_list = []\n",
    "for index, data in test_labels.iterrows():\n",
    "    moving_stopped_duration_data_list.append([data['MovingTime'], data['StoppedTime'], data['Duration']])\n",
    "moving_stopped_duration_data = np.array(moving_stopped_duration_data_list)\n",
    "moving_stopped_duration_data = np.transpose(moving_stopped_duration_data)\n",
    "\n",
    "\n",
    "test_predictions = np.transpose(model.predict(normed_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "label_texts = ['Moving Time', 'Stopped Time', 'Duration']\n",
    "    \n",
    "for i in range(3):\n",
    "    utils.scatter_plot(moving_stopped_duration_data[i], test_predictions[i], label_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrkHGKZcusUo"
   },
   "source": [
    "It looks like our model predicts reasonably well, as the results should be as close to the diagonal as possible. Let's take a look at the error distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    utils.plot_error_hist(moving_stopped_duration_data[i], test_predictions[i], label_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9_kI6MHu1UU"
   },
   "source": [
    "It's not quite gaussian, but we might expect that because the number of samples is very small.\n",
    "\n",
    "Export model for use in inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_hikingTimePrediction_simple.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "## Comparison of predictions with standard formula for predicting hiking durations\n",
    "\n",
    "In the following, the predictions are compared with the estimates for hiking durations from a standard formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.scatter_plot(moving_stopped_duration_data[2], standard_estimate_walking_time, 'Standard estimate for duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_error_hist(moving_stopped_duration_data[2], \n",
    "                      standard_estimate_walking_time, \n",
    "                      'Standard estimate for duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook presents the training of an algorithm for predicting moving and stopping times as well as total duration for hiking (but it can also be applied to many other outdoor activities). We accomplished this by treating it as a regression problem and it is demonstrated that the model predictions are significantly better than those of a standard formulate for estimating hiking times. The other two notebooks present somewhat more sophisticated models that lead to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(normed_test_data, test_labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test set results with MSE loss:\n",
    "{'loss': 392.9443359375,\n",
    " 'mean_absolute_error': 12.534769058227539,\n",
    " 'mean_squared_error': 392.9443359375}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
